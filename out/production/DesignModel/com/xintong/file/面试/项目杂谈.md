# 项目杂谈

##### 1 *线上问题oom

##### 2 *设计接口幂等性 

解决幂等性的本质就是通过各种校验，对于相同的资源不让他执行重复的动作

1. Redis+token
2. 状态机 where status = 期望的原来的status
3. 乐观锁
   - 查询数据得到版本号 version = 1
   - 在执行更新 update xxx set money = money + 100，version = version+1 where id = 1 and version = 1
4. 全局唯一编号，编号重复直接返回
5. 基于数据库做唯一主键
6. 基于数据库做唯一索引

##### 3 *第三方接口调用系列

##### 4 *分布式的缺点

##### 5 *服务器端防止表单重复提交

产生场景

- 在网络延迟的情况下让用户有时间点击多次submit按钮导致表单重复提交
- 表单提交后用户点击【刷新】按钮导致表单重复提交
- 用户提交表单后，点击浏览器的【后退】按钮回退到表单页面后进行再次提交

场景一 ：浏览器端点击一次后禁用提交

场景二 : 可是考虑使用重定向，重定向是二次请求，不在一个request和response内

场景三 : 可以考虑Session中存token，token匹配上就执行，token匹配不上就是重复提交(接口幂等)

##### 6 *项目中打日志的框架，错误日志如何配置

##### 7 *Session和Cooking区别

Cookie和Session都是服务器进行状态管理的产物

Cookie是将数据保存在浏览器，大小和数量都有限制，安全性不高

Session把信息存在服务端，基于Cookie传递SessionID，从而实现状态管理

##### 8 *get和post的区别

get 点击某个连接 form默认提交都是get 数据会携带在url中不安全，而且大小比较小

post 会将数据放在body中，安全性相对来说比较高，而且数据量也比较大 

##### 9 *减去库存问题处理

##### 10 *重复支付

##### 11 **日志怎么打，有什么规范**

##### 12**枚举场景**

##### 13. 转发重定向

重定向：服务器通知浏览器去访问另一个任意地址，状态码302，location消息头中保存重定向地址，是二次请求

转发：一个Web组件把没干完的事交给另外一个Web组件去做，在同一个请求响应之内



##### 14.红黑树

- 根节点黑色
- 所有的节点不是红色就是黑色
- 红色节点的叶节点必须是是黑色
- 任一节点到其叶子结点的路径包含的黑节点数目相同

##### 15.项目介绍

这个项目的话是属于物联网项目，跟我们平常的项目不太一样，他不是面向用户，而是面向设备，90%以上的数据都是从设备采集上来的，主要的功能就是对他的分布式电站里面的中控、机柜、电池、环境信息做一个监控以及远程配置，我在项目里面主要负责两个模块，一个是远程配置，一个是数据展示

```
我先说一下这个远程配置，我这边通过kafka发送一个消息，实际上这个消息就是一个配置指令，这个消息会经过一个kafka的代理层，这个代理层会把kafka转换成mqtt协议，mqtt传到设备的时候在把它转成MUDBUS，然后嵌入式那边会解析这个mudbus协议，从而再完成设备的配置，也就是说这个中间经历的协议转换数据流转的链路比较多，这样就容易导致数据的丢失，数据一旦丢失，就可能造成设备的实际数据和我软件这边数据库的数据不止一致那我说一下我的解决方案，我这边发送的是可靠性消息，但是我最多也只能保证我这边的指令下发成功，我不能保证你中间链路是否会有消息丢失，或者说你配置设备的时候嵌入式那边出了什么异常，这个我是没有办法保证的，所以我在下发指令之前，先把数据库中原来的数据读出来，存到ThreadLocal里面，然后呢发送指令，指令发送完成之后呢我在zk里面创建并监听一个节点，然后让他们设备完成配置之后去操作这个zk的节点，我去监听zk节点数据的变更如果他往节点力写1，那就是设备那边配置成功，那么那边也是操作成功的，也就是数据是一致的，流程到这里就完了，但如果那边往zk里写了-1，那就是设备配置失败，或者说在我们规定的那个超时时间内没有操作zk，那就是这个指令在传输的过程中某个环节弄丢了，那我我会启动数据补偿机制，把我ThreadLocal里面设备原有的值拿出来，然后恢复到数据库里面，这个算是保障数据一致性的第一道防线，还有第二道防线
第二道防线，是这样的，做一个定时任务，每一分钟，把它那边配置值读取过来，写入我这边的数据库，这种方式是我们公司做这种项目一直都采用的方式，因为他很直观，很好理解，但是这种方式有一个很大的缺点，就是一次性采集所有配置数据量太大，数据量大也就意味着丢包的可能性更大，所以呢我对他们之前的这种方式做了一个优化，解决了这个问题
刚好我那段时间再看Eureka的源码，eureka客户端从eureka服务端获取服务的时候，首次它是全量获取，后期呢是增量获取，这样用增量获取的话数据量要小得多，但是他又怕这个增量获取的不对，所以他在获取增量的时候，把增量传过去，
在传过去一个hashcode，这个hashcode是服务端全量的经过计算的hashcode，然后客户端同步了增量之后，在去计算一下客户端所有服务的hashcode，看是否和服务端传过来的这个hashcode一致，一致的话说明没什么问题，如果hashcode不一致的时候，说明增量获取的有问题，那么它再进行一次全量获取。我把eureka源码的这种思想，利用到了我们的项目中，也就是对保障数据一致性的第二道防线做了优化，我是这样做的，我用zk去监听每个配置对应的节点，如果这个节点发生了变动，那我只去获取对应的配置的值，然后让他们那边同时计算出一个全量的hashcode，我把增量变动配置拿过来之后，读取数据库中所有的配置，把这个增量配置加进去，计算一下新的全量的hashcode，跟传过来设备的hashcode做比较，一致的话那就没问题，不一致的话，那就重新获取全量配置
```

